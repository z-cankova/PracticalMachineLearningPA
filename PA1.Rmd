---
output:
  html_document:
    keep_md: yes
---
## Programming Assigment 1
## A Random Forest Model for Predicting the Correctness of Dumbbell Lift Exercises
### Author: Zdravka Cankova 
#### Data: Weight Lifting Exercises Dataset from HAR Website (http://groupware.les.inf.puc-rio.br/har) 
#### Source: Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.  

---

#### Overview

The purpose of this project was to use the weightlifting exercises dataset from the HAR website to build a model predicting the correctness of performed exercises. A random forest model with 5-fold cross-validation was built, with a resulting accuracy of **0.9863** and out-of-sample error of **0.0113**. 

#### Data Analysis

```{r, echo = FALSE, warning=FALSE, include = FALSE}
library(randomForest)
library(caret)
library(RANN)
library(doParallel)

pml_training <- read.csv("~/Documents/Coursera/Practical Machine Learning/Project/pml-training.csv")
pml_testing <- read.csv("~/Documents/Coursera/Practical Machine Learning/Project/pml-testing.csv")
```
First, I loaded the provided datasets into R (code not shown). Then, after examining the data in the pml-training set, I decided to convert the relevant features that were in factor format to numeric (excluding the first 7 variables, as well as the outcome variable).

```{r, warning=FALSE}
for(i in 8:159) {
      if(class(pml_training[,i]) == "factor") {
            pml_training[,i] <- as.numeric(as.character(pml_training[,i]))
      }
}
```

The pml-training dataset was split into training (60%) and testing (40%) sets.
```{r, warning=FALSE}
set.seed(1)
inTrain <- createDataPartition(y = pml_training$classe, p = 0.6, list = FALSE)
training <- pml_training[inTrain,]
testing <- pml_training[-inTrain,]
```

Then I proceeded to select the features in the new training set that would be included in the model using the following 3 criteria:  
1. Features that could not be reproduced, such as subject name, window and timestamp variables were excluded.   Although these might have an influence on the exercise correctness, they are irrelevant, because we want the model to be able to predict correctness regardless of who the subject is, or how long the exercise lasts.  
2. Features where more than half the data were missing were excluded (imputation is inefficient in this case and could introduce bias).  
3. Features highly correlated with others were excluded (correlation coefficient > 0.75).  
```{r, warning=FALSE}
trainingSubset <- subset(training[ , 8:160], 
                         select = which(colSums(is.na(training[ , 8:160])) < 0.5*length(training$classe)))

set.seed(1)
correlationMatrix <- cor(trainingSubset[,-53])
highlyCorrelated <- findCorrelation(correlationMatrix,
                                    cutoff = 0.75)
trainingFinal <- subset(trainingSubset,
                        select = -highlyCorrelated)
```

The resulting final training dataset was used to train a random forest model using 5-fold cross-validation repeated 3 times. A random forest model was chosen because it typically results in highest accuracy.

```{r, warning=FALSE, cache = TRUE}
#Create a list of seeds, changing the seed for each resampling
set.seed(1)
seeds <- vector(mode = "list", length = 16) #length is = (n_repeats*nresampling)+1
for(i in 1:15) {seeds[[i]] <- sample.int(n = 1000, 3)} #(3 is the number of tuning parameter, mtry for rf)

seeds[[16]]<-sample.int(1000, 1) #for the last model

fitControl <- trainControl(method = "repeatedcv",
                           number = 5,  ##5-fold CV
                           repeats = 3, ##repeated 3 times
                           seeds = seeds,
                           classProbs = TRUE,
                           verboseIter =TRUE,
                           allowParallel = TRUE)

cl <- makeCluster(detectCores())
registerDoParallel(cl)
set.seed(1)
RFFit<-train(classe ~ ., method = "rf", trControl=fitControl, data = trainingFinal)
stopCluster(cl)

print(RFFit)
RFFit$finalModel
```

The accuracy of the final model (first repeat with 2 variables at each split) was **0.9863**. A confusion matrix produced as a result of cross-validation is also shown. The resulting class errors are small. Based on this matrix, the overall *expected* out-of-sample error is **0.0113**.

The most important variables are the belt yaw, the dumbbell magnetometer reading in the z-direction, the belt magnetometer reading in the z-direction, the forearm pitch, and the dumbbell roll. The importance of variables is also summarized in the figure below:
```{r, warning=FALSE, fig.height = 7, fig.width = 10}
importance <- varImp(RFFit, scale=FALSE)
plot(importance)
```

Finally, the testing data set was used to calculate the true out-of-sample error. The resulting error was **0.0113** (1 - accuracy).
```{r, warning=FALSE}
testPredictions <- predict(RFFit, newdata = testing)
confusionMatrix(testPredictions, testing$classe)
```